{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNgrams(n, text,allNgrams):\n",
    "    nGrams = {}\n",
    "    for i in range(len(text)-n+1):\n",
    "        current = text[i]\n",
    "        for j in range(i+1,n+i):\n",
    "            current += \" \" + (text[j])\n",
    "        if current not in allNgrams:\n",
    "            allNgrams.append(current)\n",
    "        if current not in nGrams:\n",
    "            nGrams[current] = 1\n",
    "        else:\n",
    "            nGrams[current] += 1\n",
    "    return nGrams,allNgrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVector(word_list, full_word_list):\n",
    "    vector = []\n",
    "    for word in full_word_list:\n",
    "        if word not in word_list:\n",
    "            vector.append(0)\n",
    "        else:\n",
    "            vector.append(word_list[word])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(word_vectors):\n",
    "    \n",
    "    for i in range(len(word_vectors)):\n",
    "        word_vectors[i] = np.array(word_vectors[i])\n",
    "        \n",
    "    numerator = word_vectors[0]\n",
    "\n",
    "    for i in range(1, len(word_vectors)):\n",
    "        numerator = numerator * word_vectors[i]\n",
    "    \n",
    "    numerator = np.sum(numerator)\n",
    "\n",
    "    denominator = 1\n",
    "\n",
    "    for i in range(len(word_vectors)):\n",
    "        denominator *= np.sqrt(np.sum(word_vectors[i] ** 2))\n",
    "\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import javalang\n",
    "\n",
    "def getTokens (file):\n",
    "    token_words = []\n",
    "\n",
    "    tokens = list(javalang.tokenizer.tokenize(file))\n",
    "    parser = javalang.parser.Parser(tokens)\n",
    "    \n",
    "    for i in tokens:\n",
    "       token_words.append(type(i).__name__)\n",
    "    \n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import javalang\n",
    "\n",
    "def getValues (file):\n",
    "    token_words = []\n",
    "\n",
    "    tokens = list(javalang.tokenizer.tokenize(file))\n",
    "    parser = javalang.parser.Parser(tokens)\n",
    "    \n",
    "    for i in tokens:\n",
    "       token_words.append(i.value)\n",
    "    \n",
    "    return token_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(lst, chunk_size):\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def duplicate(values_min, values_max):\n",
    "    div = math.ceil(len(values_max) / len(values_min))\n",
    "    chunk_size =math.ceil( len(values_max) / div)\n",
    "    values2 = split_list(values_max,chunk_size)\n",
    "\n",
    "    allNgrams = []\n",
    "    maxCos = 0\n",
    "    for i in values2:\n",
    "        nGrams1,allNgrams = getNgrams(3,values_min,allNgrams)\n",
    "        nGrams2,allNgrams = getNgrams(3,i,allNgrams)\n",
    "\n",
    "        vector1 = getVector(nGrams1,allNgrams)\n",
    "        vector2 = getVector(nGrams2,allNgrams)\n",
    "\n",
    "        aux = cosineSimilarity([vector1, vector2])\n",
    "\n",
    "        if(maxCos < aux):\n",
    "            maxCos = aux\n",
    "\n",
    "    return maxCos\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PredictionTokens (file1_string, file2_string):\n",
    "\n",
    "    allNgrams = []\n",
    "\n",
    "    token_words1 = getTokens(file1_string)\n",
    "    token_words2 = getTokens(file2_string)\n",
    "\n",
    "    maxCos = 0\n",
    "    if (len(token_words1) / len(token_words2)) > 2:\n",
    "        maxCos = (duplicate(token_words2,token_words1))\n",
    "\n",
    "    elif (len(token_words2) / len(token_words1)) > 2:\n",
    "        maxCos = (duplicate(token_words1,token_words2))\n",
    "\n",
    "    nGrams1,allNgrams = getNgrams(3,token_words1,allNgrams)\n",
    "    nGrams2,allNgrams = getNgrams(3,token_words2,allNgrams)\n",
    "\n",
    "    vector1 = getVector(nGrams1,allNgrams)\n",
    "    vector2 = getVector(nGrams2,allNgrams)\n",
    "\n",
    "    cos = cosineSimilarity([vector1, vector2])\n",
    "    if maxCos > cos:\n",
    "        return(maxCos)\n",
    "    else:\n",
    "        return(cos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PredictionValues (file1_string, file2_string):\n",
    "\n",
    "    allNgrams = []\n",
    "\n",
    "    values1 = getValues(file1_string)\n",
    "    values2 = getValues(file2_string)\n",
    "    maxCos = 0\n",
    "    if (len(values1) / len(values2)) > 2:\n",
    "        maxCos = (duplicate(values2,values1))\n",
    "\n",
    "    elif (len(values2) / len(values1)) > 2:\n",
    "        maxCos = (duplicate(values1,values2))\n",
    "\n",
    "    nGrams1,allNgrams = getNgrams(3,values1,allNgrams)\n",
    "    nGrams2,allNgrams = getNgrams(3,values2,allNgrams)\n",
    "\n",
    "    vector1 = getVector(nGrams1,allNgrams)\n",
    "    vector2 = getVector(nGrams2,allNgrams)\n",
    "\n",
    "    cos = cosineSimilarity([vector1, vector2])\n",
    "    if maxCos > cos:\n",
    "        return(maxCos)\n",
    "    else:\n",
    "        return(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getWordList(text, unique_word_list):\n",
    "    \n",
    "    text = text + \" \"\n",
    "    word = \"\"\n",
    "    unique_words = unique_word_list\n",
    "    all_words = []\n",
    "\n",
    "    for letter in text:\n",
    "        if letter != \" \" and letter != '\\n':\n",
    "            if letter.isalpha():\n",
    "                word += letter.lower()\n",
    "        else:\n",
    "            if word not in unique_words:\n",
    "                unique_words.append(word)\n",
    "            all_words.append(word)\n",
    "            word = \"\"\n",
    "        \n",
    "    return all_words, unique_words\n",
    "\n",
    "def makeMatrix(text_list, unique_word_list):\n",
    "    unique_word_count = len(unique_word_list)\n",
    "\n",
    "    matrix = np.zeros(shape = (unique_word_count, unique_word_count))\n",
    "    \n",
    "    for i in range(len(text_list[:-1])):\n",
    "        current_position = unique_word_list.index(text_list[i])\n",
    "        next_position = unique_word_list.index(text_list[i+1])\n",
    "\n",
    "        matrix[current_position][next_position] += 1\n",
    "\n",
    "    for i in range(unique_word_count):\n",
    "        if matrix[i].sum() != 0:\n",
    "            matrix[i] = matrix[i] / matrix[i].sum()\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def getWordListCode(text, unique_word_list):\n",
    "    \n",
    "    text = text + \" \"\n",
    "    unique_words = unique_word_list\n",
    "    word = \"\"\n",
    "    all_words = []\n",
    "\n",
    "    for letter in text:\n",
    "        if letter != \" \" and letter != '\\n':\n",
    "            word += letter.lower()\n",
    "        elif len(word) > 0:\n",
    "            if word[0].isalpha() and word not in unique_words:\n",
    "                word = \"variable\"\n",
    "\n",
    "            if word not in unique_words:\n",
    "                unique_words.append(word)\n",
    "\n",
    "            all_words.append(word)\n",
    "            word = \"\"\n",
    "            \n",
    "    return all_words, unique_word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def markovSImilarity(file1_string, file2_string):\n",
    "    code1 = file1_string\n",
    "    code2 = file2_string\n",
    "    \n",
    "    code1_list = getValues(code1)\n",
    "    code2_list = getValues(code2)\n",
    "\n",
    "    full_word_list_code = list(set(code1_list + code2_list))\n",
    "\n",
    "    A = makeMatrix(code1_list, full_word_list_code)\n",
    "    B = makeMatrix(code2_list, full_word_list_code)\n",
    "\n",
    "    BT = np.transpose(B)\n",
    "    C = np.matmul(BT, A)\n",
    "    prod_int = np.trace(C)\n",
    "\n",
    "    normA = np.sqrt(np.trace(np.matmul(np.transpose(A), A)))\n",
    "    normB = np.sqrt(np.trace(np.matmul(np.transpose(B), B)))\n",
    "\n",
    "    cos_ang = prod_int / (normA * normB)\n",
    "\n",
    "    '''\n",
    "    print(\"prod_int: \", prod_int)\n",
    "    print(\"normA: \", normA)\n",
    "    print(\"normB: \", normB)\n",
    "    print(\"cos_ang: \", cos_ang)\n",
    "    '''\n",
    "\n",
    "    return cos_ang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n",
      "0.8137651821862348\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('labels.csv')\n",
    "\n",
    "df['prediction'] = 0\n",
    "df\n",
    "\n",
    "data_path = './data'\n",
    "train_data_path = os.path.join(data_path, 'Train')\n",
    "java_folder_path = train_data_path + '/*.java'\n",
    "        \n",
    "file_pair_list = []\n",
    "\n",
    "for _, folder_name in enumerate(glob.glob(train_data_path + '/*/')):\n",
    "    file_pair = []\n",
    "    for _, file_name in enumerate(glob.glob(folder_name + '/*.java')):\n",
    "        file_pair.append(file_name)\n",
    "    \n",
    "    file_pair_list.append(file_pair)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "correct_tokens = 0\n",
    "correct_total = 0\n",
    "correct_markov = 0\n",
    "\n",
    "for file_pair in file_pair_list:\n",
    "    with open(file_pair[0], 'r', encoding = 'utf8') as file1, open(file_pair[1], 'r', encoding = 'utf8') as file2:\n",
    "        file1_name = os.path.basename(file_pair[0])[:-5]\n",
    "        file2_name = os.path.basename(file_pair[1])[:-5]\n",
    "\n",
    "        expected = 1\n",
    "        current_row = df.loc[(df['sub1'] == file1_name) & (df['sub2'] == file2_name)]\n",
    "\n",
    "        if len(current_row) > 0 and current_row.iloc[0]['verdict'] == 0:\n",
    "            expected = 0\n",
    "\n",
    "        file1_string = file1.read()\n",
    "        file2_string = file2.read()\n",
    "\n",
    "        \n",
    "        prediction_values = get_PredictionValues(file1_string, file2_string)\n",
    "       \n",
    "        if prediction_values > 0:\n",
    "            if (prediction_values >= 0.7 and expected == 1) or (prediction_values < 0.7 and expected == 0):\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "        prediction_tokens = get_PredictionTokens(file1_string, file2_string)\n",
    "        \n",
    "        if prediction_tokens > 0:\n",
    "            if (prediction_tokens >= 0.988 and expected == 1) or (prediction_tokens < 0.987 and expected == 0):\n",
    "                correct_tokens += 1\n",
    "                \n",
    "        if ((prediction_values >= 0.7 or prediction_tokens >= 0.988) and expected == 1) or ((prediction_values < 0.7 or prediction_tokens < 0.987) and expected == 0):\n",
    "            correct_total += 1\n",
    "        \n",
    "\n",
    "        prediction_markov = markovSImilarity(file1_string, file2_string)\n",
    "\n",
    "        if prediction_markov > 0:\n",
    "            if (prediction_markov >= 0.7 and expected == 1) or (prediction_markov < 0.7 and expected == 0):\n",
    "                correct_markov += 1\n",
    "        \n",
    "\n",
    "print(total)\n",
    "print(correct/total)\n",
    "print(correct_tokens/total)\n",
    "print(correct_total/total)\n",
    "print(correct_markov/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
